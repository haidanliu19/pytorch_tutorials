{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PYTORCH로 딥러닝하기 60분만에 끝장내기_예제로 배우는 파이토치(PYTORCH).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PYTORCH로 딥러닝하기 60분만에 끝장내기"
      ],
      "metadata": {
        "id": "5sA6XdY2yD_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "y = np.sin(x)\n",
        "\n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()"
      ],
      "metadata": {
        "id": "y94_zIUn21gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x ).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2 ).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyteTZLPtMXG",
        "outputId": "79a47f67-0196-4626-fadf-98406bed2429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 1091.1156299186953\n",
            "199 755.1918059758764\n",
            "299 524.0857723164888\n",
            "399 364.9147135982636\n",
            "499 255.16769302454276\n",
            "599 179.41645722921936\n",
            "699 127.0748907133964\n",
            "799 90.87105587912048\n",
            "899 65.80399490916949\n",
            "999 48.4306794748704\n",
            "1099 36.37806350683603\n",
            "1199 28.00879232693523\n",
            "1299 22.191918714842416\n",
            "1399 18.145462013006345\n",
            "1499 15.328176908062694\n",
            "1599 13.365067576661247\n",
            "1699 11.996069006594155\n",
            "1799 11.040651279289344\n",
            "1899 10.373380097867267\n",
            "1999 9.907024433582409\n",
            "Result: y = -0.032233709870843744 + 0.8443490929222791 x + 0.0055608513239680055 x^2 + -0.09156777048985693 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치(PyTorch): 텐서(Tensor)"
      ],
      "metadata": {
        "id": "iogG10j2ud0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device('cpu')\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device = device, dtype = dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "b = torch.randn((), device=device, dtype=dtype)\n",
        "c = torch.randn((), device=device, dtype=dtype)\n",
        "d = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # 순전파 단계: 예측값 y를 계산합니다\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # 손실(loss)을 계산하고 출력합니다\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # 가중치를 갱신합니다.\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39BxFdNyt3tR",
        "outputId": "1ae4e170-d26c-43d8-8f53-7647e3c9e4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 4015.510986328125\n",
            "199 2679.175537109375\n",
            "299 1789.4046630859375\n",
            "399 1196.713623046875\n",
            "499 801.7317504882812\n",
            "599 538.3814086914062\n",
            "699 362.7063903808594\n",
            "799 245.45521545410156\n",
            "899 167.15428161621094\n",
            "999 114.83396911621094\n",
            "1099 79.85237121582031\n",
            "1199 56.448448181152344\n",
            "1299 40.779869079589844\n",
            "1399 30.28263282775879\n",
            "1499 23.244840621948242\n",
            "1599 18.522741317749023\n",
            "1699 15.351943969726562\n",
            "1799 13.221080780029297\n",
            "1899 11.787829399108887\n",
            "1999 10.822983741760254\n",
            "Result: y = 0.026613879948854446 + 0.8207128047943115 x + -0.004591335076838732 x^2 + -0.0882057175040245 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch: 텐서(Tensor)와 autograd"
      ],
      "metadata": {
        "id": "YJqcWXLm3eFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device('cpu')\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    \n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZJDQB9FztGE",
        "outputId": "18dc99f0-20e9-4d97-8a80-c27fc19df21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 3625.2734375\n",
            "199 2426.57763671875\n",
            "299 1626.228515625\n",
            "399 1091.5406494140625\n",
            "499 734.1162719726562\n",
            "599 495.03643798828125\n",
            "699 335.0109558105469\n",
            "799 227.82546997070312\n",
            "899 155.98062133789062\n",
            "999 107.7879409790039\n",
            "1099 75.43544006347656\n",
            "1199 53.69914627075195\n",
            "1299 39.083213806152344\n",
            "1399 29.24660873413086\n",
            "1499 22.620521545410156\n",
            "1599 18.152965545654297\n",
            "1699 15.137909889221191\n",
            "1799 13.101092338562012\n",
            "1899 11.723705291748047\n",
            "1999 10.791341781616211\n",
            "Result: y = 0.029876399785280228 + 0.8233840465545654 x + -0.005154176615178585 x^2 + -0.08858566731214523 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch: 새 autograd Function 정의하기"
      ],
      "metadata": {
        "id": "k_q3oncE3-y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math \n",
        "\n",
        "class LegendrePolynomial3(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 5e-6\n",
        "for t in range(2000):\n",
        "    P3 = LegendrePolynomial3.apply\n",
        "    y_pred = a + b * P3(c + d * x)\n",
        "\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXL8X26_38t2",
        "outputId": "6c401da1-093e-4cb6-fbde-564fc1fc47ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 209.95834350585938\n",
            "199 144.66018676757812\n",
            "299 100.70249938964844\n",
            "399 71.03519439697266\n",
            "499 50.97850799560547\n",
            "599 37.403133392333984\n",
            "699 28.206867218017578\n",
            "799 21.97318458557129\n",
            "899 17.7457275390625\n",
            "999 14.877889633178711\n",
            "1099 12.93176555633545\n",
            "1199 11.610918998718262\n",
            "1299 10.71425724029541\n",
            "1399 10.10548210144043\n",
            "1499 9.692106246948242\n",
            "1599 9.411375045776367\n",
            "1699 9.220745086669922\n",
            "1799 9.091285705566406\n",
            "1899 9.003360748291016\n",
            "1999 8.943639755249023\n",
            "Result: y = -5.394172664097141e-09 + -2.208526849746704 * P3(1.367587154632588e-09 + 0.2554861009120941 x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn 모듈"
      ],
      "metadata": {
        "id": "DL_gGnrn9wRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch: nn"
      ],
      "metadata": {
        "id": "6bYg27qONlsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    model.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "linear_layer = model[0]\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "metadata": {
        "id": "8wndGtEl9mM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01da7f58-58fe-41b6-e33b-b6f85b08d322"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 497.10687255859375\n",
            "199 331.89813232421875\n",
            "299 222.59548950195312\n",
            "399 150.27806091308594\n",
            "499 102.42875671386719\n",
            "599 70.76786041259766\n",
            "699 49.817420959472656\n",
            "799 35.953643798828125\n",
            "899 26.77890968322754\n",
            "999 20.70686912536621\n",
            "1099 16.688098907470703\n",
            "1199 14.028114318847656\n",
            "1299 12.267328262329102\n",
            "1399 11.101727485656738\n",
            "1499 10.330058097839355\n",
            "1599 9.819159507751465\n",
            "1699 9.480855941772461\n",
            "1799 9.256818771362305\n",
            "1899 9.108449935913086\n",
            "1999 9.010183334350586\n",
            "Result: y = -0.0025961073115468025 + 0.8434478044509888 x + 0.0004478720366023481 x^2 + -0.09143956750631332 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch: optim"
      ],
      "metadata": {
        "id": "YH_W322NiHH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "for t in range(2000):\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    model.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "linear_layer = model[0]\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h0U1lXsiHiN",
        "outputId": "5cb95563-0b52-4fc9-e4eb-ac966249813f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 674.759521484375\n",
            "199 452.4388427734375\n",
            "299 304.4631652832031\n",
            "399 205.93035888671875\n",
            "499 140.29251098632812\n",
            "599 96.54932403564453\n",
            "699 67.38446807861328\n",
            "799 47.93082809448242\n",
            "899 34.94902038574219\n",
            "999 26.282032012939453\n",
            "1099 20.493247985839844\n",
            "1199 16.625064849853516\n",
            "1299 14.0392427444458\n",
            "1399 12.310041427612305\n",
            "1499 11.153270721435547\n",
            "1599 10.379240036010742\n",
            "1699 9.861223220825195\n",
            "1799 9.514505386352539\n",
            "1899 9.282461166381836\n",
            "1999 9.12718391418457\n",
            "Result: y = 0.01023090910166502 + 0.8424351215362549 x + -0.0017573571531102061 x^2 + -0.09129662811756134 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch: 사용자 정의 nn.Module"
      ],
      "metadata": {
        "id": "sVVjGMZAicoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "class Polynomial3(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "    def forward(self, x):\n",
        "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "    \n",
        "    def string(self):\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "model = Polynomial3()\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
        "for t in range(2000):\n",
        "    y_pred = model(x)\n",
        "\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KeGY_hhiHle",
        "outputId": "ac0d4b53-99d1-4d9b-a5fc-0307111d1be9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 4289.7958984375\n",
            "199 2949.095947265625\n",
            "299 2030.5523681640625\n",
            "399 1400.51513671875\n",
            "499 967.8756713867188\n",
            "599 670.4500122070312\n",
            "699 465.7509765625\n",
            "799 324.7137451171875\n",
            "899 227.4333038330078\n",
            "999 160.26220703125\n",
            "1099 113.83246612548828\n",
            "1199 81.70651245117188\n",
            "1299 59.45521926879883\n",
            "1399 44.028160095214844\n",
            "1499 33.322296142578125\n",
            "1599 25.885812759399414\n",
            "1699 20.715686798095703\n",
            "1799 17.118074417114258\n",
            "1899 14.612548828125\n",
            "1999 12.866193771362305\n",
            "Result: y = 0.06086656451225281 + 0.8302542567253113 x + -0.010500495322048664 x^2 + -0.08956290036439896 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)"
      ],
      "metadata": {
        "id": "pIGNYozXlhNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import random\n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "        self.d = torch.nn.Parameter(torch.randn(()))\n",
        "        self.e = torch.nn.Parameter(torch.randn(()))\n",
        "    def forward(self, x):\n",
        "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
        "        for exp in range(4, random.randint(4, 6)):\n",
        "            y = y + self.e * x ** exp\n",
        "        return y\n",
        "    \n",
        "    def string(self):\n",
        "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
        "\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "model = DynamicNet()\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
        "for t in range(30000):\n",
        "    y_pred = model(x)\n",
        "\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'Result: {model.string()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hScgWvrJiHo3",
        "outputId": "1bb3bec4-b2b4-4445-9da1-7217cce0d750"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 1460.166015625\n",
            "199 2113.19189453125\n",
            "299 1260.23486328125\n",
            "399 1059.268798828125\n",
            "499 3443.470703125\n",
            "599 935.2164306640625\n",
            "699 892.4635620117188\n",
            "799 863.6214599609375\n",
            "899 7107.908203125\n",
            "999 61923.484375\n",
            "1099 10532.5107421875\n",
            "1199 1412.1810302734375\n",
            "1299 698.7078857421875\n",
            "1399 704.0103759765625\n",
            "1499 680.237060546875\n",
            "1599 656.8225708007812\n",
            "1699 822.0445556640625\n",
            "1799 614.1790161132812\n",
            "1899 1245.191650390625\n",
            "1999 659.97216796875\n",
            "2099 562.4525756835938\n",
            "2199 608.2973022460938\n",
            "2299 517.9310302734375\n",
            "2399 529.1103515625\n",
            "2499 472.28741455078125\n",
            "2599 458.1561279296875\n",
            "2699 486.37603759765625\n",
            "2799 439.2335205078125\n",
            "2899 410.04388427734375\n",
            "2999 398.82171630859375\n",
            "3099 395.3590087890625\n",
            "3199 378.1028747558594\n",
            "3299 357.55950927734375\n",
            "3399 360.342041015625\n",
            "3499 355.5804748535156\n",
            "3599 330.61627197265625\n",
            "3699 328.99847412109375\n",
            "3799 313.289306640625\n",
            "3899 610.9669799804688\n",
            "3999 292.3359375\n",
            "4099 328.7821960449219\n",
            "4199 258.6946716308594\n",
            "4299 265.078369140625\n",
            "4399 256.62200927734375\n",
            "4499 248.19651794433594\n",
            "4599 276.6540832519531\n",
            "4699 231.12733459472656\n",
            "4799 216.9900360107422\n",
            "4899 217.6621856689453\n",
            "4999 211.27432250976562\n",
            "5099 203.781494140625\n",
            "5199 207.7136993408203\n",
            "5299 187.30374145507812\n",
            "5399 182.6444091796875\n",
            "5499 172.22262573242188\n",
            "5599 171.05934143066406\n",
            "5699 161.6547088623047\n",
            "5799 239.5765380859375\n",
            "5899 157.45449829101562\n",
            "5999 3408.9443359375\n",
            "6099 145.81683349609375\n",
            "6199 155.36758422851562\n",
            "6299 141.31729125976562\n",
            "6399 132.68679809570312\n",
            "6499 132.14236450195312\n",
            "6599 124.27465057373047\n",
            "6699 121.16166687011719\n",
            "6799 117.5501708984375\n",
            "6899 112.56635284423828\n",
            "6999 105.26365661621094\n",
            "7099 105.8143310546875\n",
            "7199 103.42123413085938\n",
            "7299 100.25846862792969\n",
            "7399 93.60236358642578\n",
            "7499 94.54146575927734\n",
            "7599 93.68672943115234\n",
            "7699 88.43986511230469\n",
            "7799 85.19725036621094\n",
            "7899 83.26431274414062\n",
            "7999 78.2915267944336\n",
            "8099 77.399658203125\n",
            "8199 75.5216064453125\n",
            "8299 73.51365661621094\n",
            "8399 69.71595001220703\n",
            "8499 69.44303131103516\n",
            "8599 67.24244689941406\n",
            "8699 66.1484603881836\n",
            "8799 65.40361022949219\n",
            "8899 61.6346549987793\n",
            "8999 86.58775329589844\n",
            "9099 58.038761138916016\n",
            "9199 452.84033203125\n",
            "9299 54.84514236450195\n",
            "9399 51.54395294189453\n",
            "9499 49.967559814453125\n",
            "9599 49.673072814941406\n",
            "9699 47.69981002807617\n",
            "9799 47.3977165222168\n",
            "9899 45.95207595825195\n",
            "9999 44.82643127441406\n",
            "10099 43.6268196105957\n",
            "10199 46.489418029785156\n",
            "10299 101.21961212158203\n",
            "10399 40.21990203857422\n",
            "10499 39.145023345947266\n",
            "10599 38.2834587097168\n",
            "10699 37.136924743652344\n",
            "10799 36.18935775756836\n",
            "10899 35.7977294921875\n",
            "10999 34.369728088378906\n",
            "11099 33.929351806640625\n",
            "11199 32.31961441040039\n",
            "11299 31.877666473388672\n",
            "11399 31.21963119506836\n",
            "11499 29.627079010009766\n",
            "11599 29.571575164794922\n",
            "11699 29.01282501220703\n",
            "11799 29.92350196838379\n",
            "11899 28.39838218688965\n",
            "11999 26.867355346679688\n",
            "12099 26.272724151611328\n",
            "12199 25.68576431274414\n",
            "12299 25.099695205688477\n",
            "12399 24.55962562561035\n",
            "12499 24.27086067199707\n",
            "12599 23.935564041137695\n",
            "12699 22.582780838012695\n",
            "12799 22.022397994995117\n",
            "12899 22.053604125976562\n",
            "12999 22.746753692626953\n",
            "13099 21.43509292602539\n",
            "13199 21.40854263305664\n",
            "13299 20.285106658935547\n",
            "13399 20.25326919555664\n",
            "13499 19.6693172454834\n",
            "13599 19.26729965209961\n",
            "13699 18.575336456298828\n",
            "13799 18.94162368774414\n",
            "13899 18.121450424194336\n",
            "13999 17.492565155029297\n",
            "14099 17.558456420898438\n",
            "14199 17.259136199951172\n",
            "14299 16.972801208496094\n",
            "14399 16.69434928894043\n",
            "14499 16.42466926574707\n",
            "14599 34.11444091796875\n",
            "14699 15.915973663330078\n",
            "14799 15.678535461425781\n",
            "14899 15.415899276733398\n",
            "14999 15.199836730957031\n",
            "15099 14.999290466308594\n",
            "15199 14.854864120483398\n",
            "15299 14.64134693145752\n",
            "15399 14.71773910522461\n",
            "15499 14.202591896057129\n",
            "15599 14.197811126708984\n",
            "15699 13.838705062866211\n",
            "15799 13.666984558105469\n",
            "15899 13.505666732788086\n",
            "15999 13.611839294433594\n",
            "16099 13.187854766845703\n",
            "16199 13.147095680236816\n",
            "16299 12.875508308410645\n",
            "16399 14.076759338378906\n",
            "16499 12.62123966217041\n",
            "16599 12.495440483093262\n",
            "16699 12.37753677368164\n",
            "16799 12.5122652053833\n",
            "16899 12.13039779663086\n",
            "16999 12.014904022216797\n",
            "17099 12.00611686706543\n",
            "17199 11.800068855285645\n",
            "17299 11.69548225402832\n",
            "17399 11.686429023742676\n",
            "17499 11.506933212280273\n",
            "17599 11.332871437072754\n",
            "17699 11.325007438659668\n",
            "17799 12.132170677185059\n",
            "17899 11.155132293701172\n",
            "17999 11.315587043762207\n",
            "18099 14.924172401428223\n",
            "18199 11.009220123291016\n",
            "18299 10.852972030639648\n",
            "18399 10.765788078308105\n",
            "18499 10.715810775756836\n",
            "18599 10.75554084777832\n",
            "18699 10.672904968261719\n",
            "18799 10.554458618164062\n",
            "18899 10.503463745117188\n",
            "18999 10.320796012878418\n",
            "19099 10.358414649963379\n",
            "19199 10.410390853881836\n",
            "19299 10.154142379760742\n",
            "19399 10.24095630645752\n",
            "19499 10.157681465148926\n",
            "19599 10.173200607299805\n",
            "19699 10.07305908203125\n",
            "19799 10.025928497314453\n",
            "19899 10.192902565002441\n",
            "19999 10.272622108459473\n",
            "20099 9.907944679260254\n",
            "20199 10.116019248962402\n",
            "20299 9.844168663024902\n",
            "20399 9.8060884475708\n",
            "20499 9.617724418640137\n",
            "20599 9.794065475463867\n",
            "20699 9.710308074951172\n",
            "20799 9.677478790283203\n",
            "20899 9.64309310913086\n",
            "20999 9.618270874023438\n",
            "21099 9.591370582580566\n",
            "21199 9.614133834838867\n",
            "21299 9.411971092224121\n",
            "21399 9.384913444519043\n",
            "21499 9.606435775756836\n",
            "21599 9.47376537322998\n",
            "21699 9.438094139099121\n",
            "21799 9.657057762145996\n",
            "21899 9.363441467285156\n",
            "21999 9.387702941894531\n",
            "22099 9.36091423034668\n",
            "22199 9.387248992919922\n",
            "22299 9.215217590332031\n",
            "22399 9.367018699645996\n",
            "22499 9.405915260314941\n",
            "22599 9.284724235534668\n",
            "22699 9.282010078430176\n",
            "22799 9.031387329101562\n",
            "22899 9.266547203063965\n",
            "22999 9.063002586364746\n",
            "23099 9.206392288208008\n",
            "23199 9.193503379821777\n",
            "23299 9.187265396118164\n",
            "23399 9.022083282470703\n",
            "23499 10.813271522521973\n",
            "23599 8.95969009399414\n",
            "23699 8.955142974853516\n",
            "23799 9.121407508850098\n",
            "23899 8.911487579345703\n",
            "23999 9.112868309020996\n",
            "24099 9.115378379821777\n",
            "24199 9.012853622436523\n",
            "24299 8.867499351501465\n",
            "24399 9.074094772338867\n",
            "24499 8.973522186279297\n",
            "24599 9.064245223999023\n",
            "24699 9.086875915527344\n",
            "24799 8.824445724487305\n",
            "24899 9.071749687194824\n",
            "24999 8.81883430480957\n",
            "25099 8.837024688720703\n",
            "25199 9.038145065307617\n",
            "25299 9.028133392333984\n",
            "25399 9.210067749023438\n",
            "25499 9.033201217651367\n",
            "25599 8.751468658447266\n",
            "25699 8.749258041381836\n",
            "25799 8.96883773803711\n",
            "25899 8.714348793029785\n",
            "25999 8.746520042419434\n",
            "26099 8.985980987548828\n",
            "26199 8.984184265136719\n",
            "26299 8.717000961303711\n",
            "26399 8.949836730957031\n",
            "26499 8.970969200134277\n",
            "26599 8.984918594360352\n",
            "26699 8.676563262939453\n",
            "26799 8.954754829406738\n",
            "26899 8.928081512451172\n",
            "26999 8.956250190734863\n",
            "27099 8.920839309692383\n",
            "27199 8.916110038757324\n",
            "27299 8.92313003540039\n",
            "27399 8.916569709777832\n",
            "27499 8.64247989654541\n",
            "27599 8.951428413391113\n",
            "27699 8.929516792297363\n",
            "27799 8.709221839904785\n",
            "27899 8.907096862792969\n",
            "27999 8.959858894348145\n",
            "28099 8.90872573852539\n",
            "28199 8.902020454406738\n",
            "28299 8.701008796691895\n",
            "28399 8.691691398620605\n",
            "28499 8.895134925842285\n",
            "28599 8.901215553283691\n",
            "28699 8.900288581848145\n",
            "28799 9.04411792755127\n",
            "28899 8.660514831542969\n",
            "28999 9.141926765441895\n",
            "29099 8.924988746643066\n",
            "29199 8.884974479675293\n",
            "29299 8.632488250732422\n",
            "29399 8.882654190063477\n",
            "29499 8.940146446228027\n",
            "29599 8.941495895385742\n",
            "29699 8.882784843444824\n",
            "29799 8.928606033325195\n",
            "29899 8.585762977600098\n",
            "29999 8.580604553222656\n",
            "Result: y = -0.005853024777024984 + 0.8571484088897705 x + 0.0004266045580152422 x^2 + -0.09376750886440277 x^3 + 0.00013140251394361258 x^4 ? + 0.00013140251394361258 x^5 ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_nbnwEfaiHr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}