{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "동적 양자화.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "출처 : pytorch 튜트리얼"
      ],
      "metadata": {
        "id": "Kcrm0WtMOD2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.quantization\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "yJWwkdzFc8Yx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class lstm_for_demonstration(nn.Module):\n",
        "    def __init__(self,in_dim,out_dim,depth):\n",
        "        super(lstm_for_demonstration,self).__init__()\n",
        "        self.lstm = nn.LSTM(in_dim,out_dim,depth)\n",
        "\n",
        "    def forward(self,inputs,hidden):\n",
        "        out,hidden = self.lstm(inputs,hidden)\n",
        "        return out, hidden\n"
      ],
      "metadata": {
        "id": "ymivgnsTwCMK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(29592)\n",
        "\n",
        "model_dimension=8\n",
        "sequence_length=20\n",
        "batch_size=1\n",
        "lstm_depth=1\n",
        "\n",
        "inputs = torch.randn(sequence_length,batch_size,model_dimension)\n",
        "hidden = (torch.randn(lstm_depth,batch_size,model_dimension), torch.randn(lstm_depth,batch_size,model_dimension))"
      ],
      "metadata": {
        "id": "vdjUdbVPw7xq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "float_lstm = lstm_for_demonstration(model_dimension, model_dimension, lstm_depth)\n",
        "\n",
        "quantized_lstm =  torch.quantization.quantize_dynamic(\n",
        "    float_lstm, {nn.LSTM, nn.Linear}, dtype = torch.qint8\n",
        ")\n",
        "\n",
        "print('Here is the floating point version of this module:')\n",
        "print(float_lstm)\n",
        "print('')\n",
        "print('and now the quantized version:')\n",
        "print(quantized_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi2ClfVuyT_3",
        "outputId": "f01fa587-e491-45e9-fb5e-d2b0252d74ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the floating point version of this module:\n",
            "lstm_for_demonstration(\n",
            "  (lstm): LSTM(8, 8)\n",
            ")\n",
            "\n",
            "and now the quantized version:\n",
            "lstm_for_demonstration(\n",
            "  (lstm): DynamicQuantizedLSTM(8, 8)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model, label = \"\"):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\")\n",
        "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "f=print_size_of_model(float_lstm,\"fp32\")\n",
        "q=print_size_of_model(quantized_lstm,\"int8\")\n",
        "print(\"{0:.2f} times smaller\".format(f/q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogqOUVWQx7Wq",
        "outputId": "e55ddf7e-92ca-4c9b-d92c-ceb281f3a181"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model:  fp32  \t Size (KB): 3.743\n",
            "model:  int8  \t Size (KB): 2.719\n",
            "1.38 times smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Floating point FP32\")\n",
        "%timeit float_lstm.forward(inputs, hidden)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GALLF-jyJsr",
        "outputId": "aa700fc6-5b33-42c0-ed19-a34a65086a0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Floating point FP32\n",
            "The slowest run took 70.56 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 950 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Quantized INT8\")\n",
        "%timeit quantized_lstm.forward(inputs,hidden)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ig4hdfVzm1L",
        "outputId": "63cbf657-40bd-41bb-adaa-d5e0bf6a0e9d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized INT8\n",
            "The slowest run took 25.55 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 621 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1, hidden1 = float_lstm(inputs, hidden)\n",
        "mag1 = torch.mean(abs(out1)).item()\n",
        "print('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n",
        "\n",
        "out2, hidden2 = quantized_lstm(inputs, hidden)\n",
        "mag2 = torch.mean(abs(out2)).item()\n",
        "print('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n",
        "\n",
        "mag3 = torch.mean(abs(out1-out2)).item()\n",
        "print('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afl5XouhzuP7",
        "outputId": "62775e55-1c9b-4ddc-b2b3-fc14c0fc245f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean absolute value of output tensor values in the FP32 model is 0.12887 \n",
            "mean absolute value of output tensor values in the INT8 model is 0.12912\n",
            "mean absolute value of the difference between the output tensors is 0.00156 or 1.21 percent\n"
          ]
        }
      ]
    }
  ]
}